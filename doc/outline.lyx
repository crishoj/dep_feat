#LyX 1.6.3 created this file. For more info see http://www.lyx.org/
\lyxformat 345
\begin_document
\begin_header
\textclass report
\begin_preamble
\usepackage{url}
\end_preamble
\use_default_options true
\language english
\inputencoding utf8
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_amsmath 1
\use_esint 1
\cite_engine natbib_authoryear
\use_bibtopic false
\paperorientation portrait
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\defskip medskip
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\author "" 
\author "" 
\end_header

\begin_body

\begin_layout Title
Feature Engineering in Data-Driven Dependency Parsing
\end_layout

\begin_layout Author
Christian Rishøj Jensen
\end_layout

\begin_layout Standard
\begin_inset CommandInset toc
LatexCommand tableofcontents

\end_inset


\end_layout

\begin_layout Chapter
Introduction
\end_layout

\begin_layout Standard
Language competence might well be among the most impressive of cognitive
 abilities, both with respect to the conceptual complexity conveyable in
 the linguistic vehicle, and the effectiveness with which we are able to
 grasp utterances and make sense of them.
 Thanks to the generative quality of language, by which phrases and sentences
 may be combined and embedded within each other recursively, the extension
 of language is potentially limitless.
 On the comprehension side, structureally ambigous and semantically underspecifi
ed utterances are swiftly decoded and assigned with a likely interpretation.
 A task that from a formal point of view has immense complexity is continually
 carried out by the language user, seemingly without any effort.
 During this process the comprehender presumably registers a variety of
 phenomena in the uttered speech or written text in order to contruct the
 proper interpretation.
 As examplars of this, apart from the sequence and identity of the uttered
 words themselves, the comprehender is likely to take of the 
\emph on
kind
\emph default
 of word being used, i.e.
 the 
\emph on
part of speech
\emph default
 to which the word belongs, 
\emph on
morphological
\emph default
 structure of the words themselves, as well as grammatical pauses in between
 words, and employ this information in interpretation.
 I might not know the word 
\emph on
flocculation
\emph default
, but from its suffix of 
\emph on
-tion
\emph default
 I can tell that it is a noun, probably for an action or a condition, which
 can help me get an idea of how it fits in a structural analysis of the
 sentence.
 Clarifying remarks in the middle of an utterence might be indicated by
 a brief pause.
 And when my mother reports on a heated discussion with a salesperson, a
 clear change in tone of voice provides me with an indication of when she
 is 
\emph on
directly reporting
\emph default
 the other person's speech.
\end_layout

\begin_layout Standard
EXPAND: quotations in Danish?
\end_layout

\begin_layout Standard
In the field of human language technology, a computer system that performs
 a syntactic, semantic, or other level of structural analysis of a sentence
 is referred to as a 
\emph on
parser
\emph default
.
 Modern day parsing systems also benefit from information other than the
 surface form of processed words.
 A common step prior to parsing is to identify the part of speech for each
 word.
 Tense, case, number, gender and other inflectional information is often
 also extracted and made explicit, in order to make the information carried
 herein available for the parser to take advantage of it when making decisions
 in the parsing process.
 
\end_layout

\begin_layout Standard
One approach to automatic syntactic analysis, referred to as 
\emph on
dependency parsing
\emph default
, has gained increasing interest in the last decade.
 While neither dependency parsing nor the underlying formalism of 
\emph on
dependency grammer
\emph default
 are new inventions, the intense interest likely stems from the success
 researches have had in constructing dependency parsers that do not rely
 on hand-crafted grammatical rules and lexica, but rather 
\emph on
learn
\emph default
 from vast amounts of example analyses.
 This training material, referred to as a 
\emph on
treebank,
\emph default
 consists of collections of sentences and corresponding structural analyses.
 Such 
\emph on
data-driven dependency parsers
\emph default
 have quickly risen to become competitive with traditional grammar-based
 systems in parsing accuracy, without the immensely time-consuming task
 of manually crafting a grammar.
 There is optimism in the research community that a hybrid of both worlds,
 in which a combination of different parsers allows one to benefit from
 the analysis of the other, will lead to even better results.
\end_layout

\begin_layout Standard
While data-driven parsers are able to learn their trade from examples, they
 are not cognitive models of human language ability, that is to say, approximati
ons of human cognitive processes for the purpose of comprehension and prediction.
 Parsing systems are constructed with other goals in mind, predominantly
 automated language processing for such tasks as information retrieval,
 document classification and summarization, machine translation and automated
 dialog systems.
\end_layout

\begin_layout Standard
However, as parsing systems already benefit from cues such as inflectional
 features and part of speech, it does not seem far-fetched to assume that
 parsing could benefit from other cues that are of use in human apprehension.
\end_layout

\begin_layout Standard
The aim of this thesis shall be to investigate the following question: Is
 it possible to augment treebanks with additional or modified features,
 such that existing data-driven parsing systems generate better dependency
 parsers?
\end_layout

\begin_layout Standard
In parsing system research, there seems to be an emphasis on constructing
 parsers for written text.
 While noone would argue that a spoken utterence from a language user turns
 into another language if she chooses to write it on a piece of paper, there
 are however substantial differences.
 Interpretational cues found in spoken language do not always have direct
 equivalents in writing.
 Variations in tone of voice for example is not obvious from a written accout.
 Some cues are present in writing, though, and while the cues in speech
 are often subtle, the cues that are present in writing stand out more clearly.
 In particular, directly reported speech is conventionally marked with quotation
 marks, and inserted supplementary remarks are typically surrounded by parenthes
es, hyphens or -- more subtly -- commas.
 Thus, there are plenty of potential features in written texts to choose
 from as well.
 
\end_layout

\begin_layout Standard
So, the focus area of this project is squarely investigating feature engineering
 in data-driven parsing systems.
 But conversely, such experiments with feature engineering in data-driven
 parsing systems may also offer an interesting opportunity to investigate
 which features of spoken or written utterances might be of value for comprehens
ion in genereal -- including human.
 Given a sufficiently capable and trainable parsing model, if we are interested
 in whether for example capitalization of written text could play any role
 in comprehension, we could train two models with the same training material,
 but make word capitalization explicit to only one of them.
 Once trained, let both models attempt an interpretation of the same text,
 and see if the extra feature provided to the latter model had any significant
 effect on the analytical performance.
 
\end_layout

\begin_layout Standard
Of couse, an observed positive effect of a feature in a parsing system would
 not entail the existence of a cognitive counterpart in use by language
 users.
 But it would provide evidence of the informational value of the cue with
 respect to structural comprehension.
\end_layout

\begin_layout Chapter
An Overview of Data-Driven Dependency Parsing
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename /Users/christian/Projects/dep_feat/doc/figures/example_dependency_tree.eps
	width 100text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Dependency structure for a Danish sentence from the Danish Dependency Treebank
 
\begin_inset CommandInset citation
LatexCommand citep
key "kromann_danish_2003"

\end_inset

.
 
\begin_inset CommandInset label
LatexCommand label
name "Flo:ex-dep"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename /Users/christian/Projects/dep_feat/doc/figures/example_constituent_structure.eps
	width 80text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
A possible constituent structure for the sentence in figure
\begin_inset CommandInset ref
LatexCommand vref
reference "Flo:ex-dep"

\end_inset

.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
Dependency Grammar
\end_layout

\begin_layout Standard
By some estimates the tradition of dependency grammar can be traced as far
 back as some of the earliest works of descriptive and generative linguistics,
 namely the Sanskrit grammar of 
\begin_inset Formula $Pāṇini$
\end_inset

 a few centuries before the Common Era.
 The tradition includes a large and diverse family of grammatical theories
 and formalisms, all sharing the basic assumption that syntactic structures
 essentially consists of 
\emph on
words
\emph default
 that are related to each other by binary, assymmetrical relations called
 
\emph on
dependencies
\emph default
 
\begin_inset CommandInset citation
LatexCommand citep
key "kbler_dependency_2009,nivre_dependency_2005"

\end_inset

, with one word in a dependency relation being the 
\emph on
head
\emph default
, and the other being the 
\emph on
dependent
\emph default
.
 
\end_layout

\begin_layout Standard
With respect to this basic property, dependency structures can be contrasted
 to 
\emph on
constituency structures
\emph default
, another dominant syntactic representation, in which words are not directly
 related to each, but only indirectly through 
\emph on
non-terminal
\emph default
 nodes, each of which may in turn be related to another non-terminal node.
 A non-terminal node in a constituency structure is not manifest in the
 surface form of the sentence, but forms a group of words, or a 
\emph on
phrase
\emph default
, which stand together as a conceptual unit.
\end_layout

\begin_layout Standard
The same conceptual units can be recognized in a dependency structure by
 their heads: All words that transitively depend on a head are part of the
 conceptual unit 
\emph on
governed
\emph default
 by the head.
 As a bit of additional nomenclature, dependents are said to 
\emph on
modify
\emph default
 their heads.
\end_layout

\begin_layout Standard
Various criteria for identifying dependency relations have been proposed.
 Some common criteria are 
\begin_inset CommandInset citation
LatexCommand citep
key "nivre_dependency_2005"

\end_inset

:
\begin_inset CommandInset label
LatexCommand label
name "Various-criteria"

\end_inset


\end_layout

\begin_layout Enumerate
A head determines the syntactic category of the dependent and can often
 replace the dependent.
\end_layout

\begin_layout Enumerate
A head determines the semantic category of the dependent.
\end_layout

\begin_layout Enumerate
Dependents provides semantic speciﬁcation.
 
\end_layout

\begin_layout Enumerate
A head is obligatory, while dependents may be optional.
 
\end_layout

\begin_layout Enumerate
The head selects its dependent and determines whether the dependent is obligator
y or optional.
 
\end_layout

\begin_layout Enumerate
The form of the dependent is determined by the head (agreement or government).
\end_layout

\begin_layout Enumerate
The linear position of a dependent is speciﬁed with reference to the head.
 
\end_layout

\begin_layout Standard
These criteria refer to a mix of phenomena on different levels of linguictics
 analysis, from morphosyntax to semantics.
 There seems to be no universally authoritative set of criteria for all
 dependency relations 
\begin_inset CommandInset citation
LatexCommand citet
key "kbler_dependency_2009"

\end_inset

.
 Rather, several levels of dependency analysis can be made on a sentence,
 and the criteria for identifying dependencies are chosen accordingly.
 Levels of dependency analysis include: 
\end_layout

\begin_layout Description
Morphosyntactic in which inflectional affixes are represented as separate
 tokens (useful for highly inflected languages).
\end_layout

\begin_layout Description
Syntactic where dependencies identify syntactic functions, including 
\noun on
predicate
\noun default
, 
\noun on
subject
\noun default
 and 
\noun on
object
\noun default
.
\end_layout

\begin_layout Description
Semantic where semantic roles (including 
\noun on
agent
\noun default
, 
\noun on
patient
\noun default
 and 
\noun on
goal
\noun default
) are designated by dependencies.
\end_layout

\begin_layout Standard
It is worth noting that dependency representation is not inherently limited
 to these levels of analysis.
 In principle, there is nothing that hinders the use of dependency analysis
 for identifying other relations between tokens for which a consistent set
 of criteria can be formulated.
 Additionally, theoretical frameworks for 
\emph on
multi-stratal
\emph default
 representations exist, in which several levels of analysis is represented
 in the same structure.
 For this project, the focus is exclusively on mono-stratal syntactic analysis.
\end_layout

\begin_layout Standard
In comparison to constituency structures, dependency structures are a more
 constrained representation, as the number of nodes to connect in the structure
 is fixed by the number of words in the sentence, whereas the constituency
 structure contains additional non-terminal nodes.
 To many researchers in computational linguistics, working with dependency
 representations has seemed more tractable and more apt for further semantic
 processing, thanks to the relatively transparent encoding of the predicate-argu
ment structure of a sentence through word dependencies 
\begin_inset CommandInset citation
LatexCommand citet
key "nivre_dependency_2005"

\end_inset

.
\end_layout

\begin_layout Subsection
Formal definitions
\end_layout

\begin_layout Standard
Following 
\begin_inset CommandInset citation
LatexCommand citet
key "kbler_dependency_2009"

\end_inset

, let a sentence 
\begin_inset Formula $S$
\end_inset

 be defined in formal terms as a sequence of tokens:
\end_layout

\begin_layout Standard
\begin_inset Formula \[
S=w_{0}w_{1}...w_{n}\]

\end_inset


\end_layout

\begin_layout Standard
Dependency structure is defined on these tokens, so tokenization of the
 sentence must have taken place prior to the dependency analysis.
 Further, let 
\begin_inset Formula $R$
\end_inset

 denote a finite set of dependency 
\emph on
relation types
\emph default
 -- or 
\emph on
arc labels
\emph default
 -- that can hold between tokens:
\end_layout

\begin_layout Standard
\begin_inset Formula \[
R=\left\{ r_{1},r_{2},...,r_{m}\right\} \]

\end_inset


\end_layout

\begin_layout Standard
A 
\emph on
dependency graph
\emph default
 
\begin_inset Formula $G$
\end_inset

 is then defined as a 
\emph on
labeled, directed 
\emph default
graph of nodes 
\begin_inset Formula $V$
\end_inset

 and arcs 
\begin_inset Formula $A$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula \[
G=(V,A)\]

\end_inset


\end_layout

\begin_layout Standard
The nodes 
\begin_inset Formula $V$
\end_inset

 in the graph are comprised of sentence tokens, and the labeled arcs 
\begin_inset Formula $A$
\end_inset

 connect these:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{eqnarray*}
V & \subseteq & \left\{ w_{0},w_{1},...,w_{n}\right\} \\
A & \subseteq & V\times R\times V\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
In these terms, a dependency graph 
\begin_inset Formula $G$
\end_inset

 represents a particular dependency analysis of the sentence 
\begin_inset Formula $S$
\end_inset

, with the arcs 
\begin_inset Formula $A$
\end_inset

 being the dependencies posited by the analysis.
 For mono-stratal analysis, we only allow a single relation to hold from
 one token to another:
\begin_inset Formula \[
(w_{i},r,w_{J})\in A\implies(w_{i},r',w_{J})\notin A\text{ for all }r'\neq r\]

\end_inset


\end_layout

\begin_layout Standard
Note that the single arc restriction in this formulation is only refers
 to one direction, namely from 
\begin_inset Formula $w_{i}$
\end_inset

 to 
\begin_inset Formula $w_{j}$
\end_inset

 (denoted by 
\begin_inset Formula $w_{i}\to w_{j}$
\end_inset

).
 It does not prohibit another relation from going the other way, 
\begin_inset Formula $w_{j}\to w_{i}$
\end_inset

.
 Additionally, there is nothing that prohibits 
\begin_inset Formula $cycles$
\end_inset

 from occurring in the the graph.
 Thus, let a 
\emph on
well-formed dependency graph
\emph default
 be any dependency graph 
\begin_inset Formula $G$
\end_inset

 of nodes 
\begin_inset Formula $V$
\end_inset

 and arcs 
\begin_inset Formula $A$
\end_inset

 that is a 
\emph on
directed tree
\emph default
 rooted in node 
\begin_inset Formula $w_{0}$
\end_inset

 that has node set 
\begin_inset Formula $V_{S}$
\end_inset

 which spans all nodes in the graph: 
\begin_inset Formula $V=V_{S}$
\end_inset

.
 Well-formed dependency graphs are also called 
\emph on
dependency trees
\emph default
.
 The tree constraint is assumed by most mono-stratal dependency theories,
 and also holds within each layer in most multi-stratal theories
\begin_inset CommandInset citation
LatexCommand citep
key "kbler_dependency_2009"

\end_inset

.
\end_layout

\begin_layout Standard
A property of dependency trees that follow from the tree constraint itself
 is the 
\emph on
single-head property
\emph default
, which states that if a dependency relation 
\begin_inset Formula $w_{i}\to w_{j}$
\end_inset

 holds between two tokens 
\begin_inset Formula $w_{i},w_{j}\in V$
\end_inset

, then there can be no other 
\begin_inset Formula $w_{i'}\in V$
\end_inset

 such that 
\begin_inset Formula $i'\neq i$
\end_inset

 and 
\begin_inset Formula $w_{i'}\to w_{j}$
\end_inset

.
 There is some controversy to this property, as there are cases where it
 seems natural to have multiple heads on a dependent.
 In particular, in sentences with coordinated verbs -- e.g.
 
\emph on
guests enter and exit through the lobby
\emph default
 -- it would appear reasonable to let the subject depend on both verbs.
 Most formalisms deal with dependents of coordinated phrases by simply letting
 the dependent modify the head of the coordinated phrase -- whether it is
 chosen to be the coordination itself or one of the coordinated tokens.
\end_layout

\begin_layout Subsection
Projectiveness
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement b
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename /Users/christian/Projects/dep_feat/doc/figures/examples/non_projective.eps
	width 100text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Non-projective dependency tree.
 The dependency 
\emph on
den 
\begin_inset Formula $\to$
\end_inset

 ordinerede
\emph default
 is crossing the dependency 
\emph on
viser
\emph default
 
\begin_inset Formula $\to$
\end_inset

 
\emph on
frem
\emph default
.
\end_layout

\end_inset


\end_layout

\end_inset

A further restriction on dependency trees is often assumed for reasons of
 computational tractability, namely 
\emph on
projectiveness
\emph default
.
 A dependency tree if projective if and only if it satisfies the 
\emph on
planar property
\emph default
, namely if it is possible to graphically configure all the dependency arcs
 in a place above the sentence -- without any crossing arcs.
 For such a depiction to be possible, the head 
\begin_inset Formula $w_{i}$
\end_inset

 of all dependencies 
\begin_inset Formula $w_{i}\to w_{j}$
\end_inset

 in the tree must dominate every node 
\begin_inset Formula $w_{m}$
\end_inset

 which occurs between the endpoints 
\begin_inset Formula $w_{i},w_{j}$
\end_inset

 in the linear order of the sentence.
 
\emph on
Dominate
\emph default
 here is the transitive and reflexive closure of the dependency relation,
 so that a node is dominated by its head, the head of its head, and so forth
 up until the root node of the tree.
 
\end_layout

\begin_layout Standard
In formal terms, this amounts the following definitions 
\begin_inset CommandInset citation
LatexCommand citep
key "kbler_dependency_2009"

\end_inset

:
\end_layout

\begin_layout Enumerate
Let 
\begin_inset Formula $w_{i}\twoheadrightarrow w_{j}$
\end_inset

 denote the reflexive and transitive closure of the dependency relation
 in a tree 
\begin_inset Formula $G=(V,A)$
\end_inset

: 
\begin_inset Formula $w_{i}\twoheadrightarrow w_{j}$
\end_inset

 if and only if 
\begin_inset Formula $i=j$
\end_inset

 (reflexive) or both 
\begin_inset Formula $w_{i}\twoheadrightarrow w_{i'}$
\end_inset

 and 
\begin_inset Formula $w_{i}\to w_{j}$
\end_inset

 for some 
\begin_inset Formula $w_{i'}\in V$
\end_inset

.
\end_layout

\begin_layout Enumerate
An arc 
\begin_inset Formula $(w_{i},r,w_{j})\in A$
\end_inset

 in a dependency tree 
\begin_inset Formula $G=(V,A)$
\end_inset

 is projective if and only if 
\begin_inset Formula $w_{i}\twoheadrightarrow w_{k}$
\end_inset

 for all 
\begin_inset Formula $i<k<j$
\end_inset

 when 
\begin_inset Formula $i<j$
\end_inset

, or 
\begin_inset Formula $j<k<i$
\end_inset

 when 
\begin_inset Formula $j<i$
\end_inset

.
\end_layout

\begin_layout Enumerate
A dependency tree 
\begin_inset Formula $G=(V,A)$
\end_inset

 is a projective dependency tree if all 
\begin_inset Formula $(w_{i},r,w_{j})\in A$
\end_inset

 are projective.
 
\end_layout

\begin_layout Subsubsection
Projectivization
\end_layout

\begin_layout Standard
Non-projective dependencies can be caused by wh-movement, which occurs naturally
 in many languages with SVO as dominant order.
 As such, projectiveness is not a linguistically plausible constraint.
 In cases where it nonetheless is assumed, non-projective cases are normally
 handled by having procedures for converting non-projective trees to projective
 trees (
\emph on
projectivization
\emph default
) and back (
\emph on
de-projectivization
\emph default
) as steps of pre- and post-processing (see figure 
\begin_inset CommandInset ref
LatexCommand vref
reference "fig:Projectivization"

\end_inset

).
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename figures/examples/projectivization.pdf

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Projectivization of a non-projective dependency tree 
\begin_inset CommandInset citation
LatexCommand citep
key "kbler_dependency_2009"

\end_inset

.
\begin_inset CommandInset label
LatexCommand label
name "fig:Projectivization"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Section
Dependency Parsing
\end_layout

\begin_layout Standard
In this section I aim to outline of the problem dependency parsing, with
 the emphasis on data-driven parsing and the learning process entailed.
\end_layout

\begin_layout Standard
Simply put, dependency parsing is the task of constructing a dependency
 tree for a sentence.
 Approaches to the parsing problem can be characterized by being 
\emph on
grammar-based
\emph default
 or 
\emph on
data-driven
\emph default
.
 An approach is grammar-based if a formally specified and often hand-crafted
 grammar is employed in parsing, such that it makes sense to ask whether
 a given sentence is producable by the grammar or not.
 In most data-driven approaches, a method from 
\emph on
machine learning
\emph default
 is used to train a parsing model on a collection of example analyzed sentences,
 resulting in some 
\emph on
learned parameters
\emph default
, which in turn is used by the parsing model to parse new sentences.
 It is worth noting that these categories are not mutually exclusive, as
 some approaches induce grammars from collections of analyzed sentences,
 and this are indeed both data-driven and grammar-based.
\end_layout

\begin_layout Standard
In formal terms, a dependency parsing model 
\begin_inset Formula $M$
\end_inset

 consists of:
\end_layout

\begin_layout Enumerate
a set of 
\emph on
constraints
\emph default
 
\begin_inset Formula $\Gamma$
\end_inset

 which define the space of possible dependency structures producable for
 a sentence, 
\end_layout

\begin_layout Enumerate
a fixed 
\emph on
algorithm
\emph default
 
\begin_inset Formula $h$
\end_inset

, and
\end_layout

\begin_layout Enumerate
a -- possibly empty -- set of 
\emph on
parameters
\emph default
 
\begin_inset Formula $\lambda$
\end_inset

 that guide the parsing algorithm.
\end_layout

\begin_layout Standard
For a grammar-based system, the constraints would include the rules of grammar
 in use.
 
\end_layout

\begin_layout Standard
The parsing problem in formal terms is then to find the most likely dependency
 tree 
\begin_inset Formula $G=h(S,\Gamma,\lambda)$
\end_inset

 for a sentence 
\begin_inset Formula $S$
\end_inset

.
 Which tree is most likely depends, apart from the general criteria for
 dependencies outlined 
\begin_inset CommandInset ref
LatexCommand vpageref
reference "Various-criteria"

\end_inset

, on the specific dependency formalism in use.
 In between formalisms there are various annotational conventions, not only
 with respect to the set of dependency types used, but also differences
 in treatmeant of such phenomena as coordination 
\begin_inset CommandInset citation
LatexCommand citep
key "nivre_dependency_2005"

\end_inset

, head-status of functional categories 
\begin_inset CommandInset citation
LatexCommand citep
key "vrelid_cross-lingual_2009"

\end_inset

 and locutions 
\begin_inset CommandInset citation
LatexCommand citep
key "bosco_linguistic_2008"

\end_inset

, which are particularly open to interpretation from a dependency viewpoint.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename /Users/christian/Projects/dep_feat/doc/figures/examples/coordination1.pdf
	width 45text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Coordination as head
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename /Users/christian/Projects/dep_feat/doc/figures/examples/coordination2.pdf
	width 45text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Coordinated term as head
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Caption

\begin_layout Plain Layout
Different conventions for annotating coordinated noun phrases 
\begin_inset CommandInset citation
LatexCommand citep
key "nivre_dependency_2005"

\end_inset

.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
A data-driven parser must learn these annotational conventions when presented
 with the training material.
 In formal terms, the training material 
\begin_inset Formula $\mathcal{D}$
\end_inset

 is comprised of pairs of sentences and dependency trees 
\begin_inset CommandInset citation
LatexCommand citep
key "kbler_dependency_2009"

\end_inset

:
\begin_inset Formula \[
\mathcal{D}=\left\{ (S_{d},G_{d})\right\} _{d=0}^{|\mathcal{D}|}\]

\end_inset


\end_layout

\begin_layout Standard
The core of the learning phase typically involves adjusting the parameters
 
\begin_inset Formula $\lambda$
\end_inset

 in order to optimize some function over the training set 
\begin_inset Formula $\mathcal{D}$
\end_inset

.
 The nature of the parameters depend on the specific parsing model in use.
 In one model, a parameter might represent the likelihood of a dependency
 arc occurring in a sentence.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename figures/data-driven/Training_Phase.eps
	lyxscale 50
	scale 60

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Learning phase
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename figures/data-driven/Parsing_Phase.eps
	lyxscale 50
	scale 60

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Parsing phase
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Phases of data-driven parsing.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
TODO: root token
\end_layout

\begin_layout Subsection
Feature function
\end_layout

\begin_layout Standard
Also involved in the learning phase is a 
\emph on
feature function
\emph default
 
\begin_inset Formula $f(x):\mathcal{X}\to\mathcal{Y}$
\end_inset

, that maps some input 
\begin_inset Formula $x$
\end_inset

 into the feature space 
\begin_inset Formula $\mathcal{Y}$
\end_inset

.
 The specific input for the feature function depends on the parsing model,
 but almost always includes a token from the sentence, and in some cases
 the state of the parser.
 In broad terms, the feature function takes information about a sentence
 that is deemed useful for the purpose learning and parsing, and makes it
 explicit to the machine learning component, typically in the form of a
 high-dimensional real-valued feature vector.
 
\end_layout

\begin_layout Standard
Example features for a given token include:
\end_layout

\begin_layout Itemize
Word form.
\end_layout

\begin_layout Itemize
Lemma.
\end_layout

\begin_layout Itemize
Part of speech.
\end_layout

\begin_layout Itemize
Morphological and inflectional properties.
\end_layout

\begin_layout Itemize
Similar information about the other tokens surrounding the particular token.
\end_layout

\begin_layout Subsection
Transition-based models
\end_layout

\begin_layout Standard
Transition-based models process sentence tokens sequentially in a stepwise
 manner, and construct a dependency tree for the sentence as a side-effect
 of the 
\emph on
transitions
\emph default
 made in an 
\emph on
abstract machine
\emph default
 as the parsing proceeds.
 At each step, the machine is in a certain 
\emph on
state
\emph default
 determined by the position in the sentencen and the 
\emph on
parsing history
\emph default
, ie.
 the transitions performed prior to the current state.
 
\end_layout

\begin_layout Subsubsection
Shift-reduce models
\end_layout

\begin_layout Standard
Several transition-based models have been proposed 
\begin_inset CommandInset citation
LatexCommand citep
key "nivre_conll_2007"

\end_inset

, many of which are inspired by traditional shift-reduce parsers
\begin_inset CommandInset citation
LatexCommand citep
key "nivre_efficient_2003,hall_single_2007"

\end_inset

.
 In these, the state -- or 
\emph on
configuration -- 
\emph default
of the transition-based parser for a sentence 
\begin_inset Formula $S$
\end_inset

 is a triple 
\begin_inset Formula $c=(\sigma,\beta,A)$
\end_inset

 of:
\end_layout

\begin_layout Enumerate
A stack 
\begin_inset Formula $\alpha$
\end_inset

 of tokens 
\begin_inset Formula $w_{i}\in V_{S}$
\end_inset

,
\end_layout

\begin_layout Enumerate
a buffer 
\begin_inset Formula $\beta$
\end_inset

 of tokens 
\begin_inset Formula $w_{i}\in V_{S}$
\end_inset

 and
\end_layout

\begin_layout Enumerate
a set 
\begin_inset Formula $A$
\end_inset

 of dependency arcs 
\begin_inset Formula $(w_{i},r,w_{j})\in V_{S}\times R\times V_{S}$
\end_inset

.
\end_layout

\begin_layout Standard
As the transition-based parser proceeds, the state contains a partial analysis
 of the sentence, where 
\begin_inset Formula $A$
\end_inset

 contains arcs of the partially completed dependency tree, the buffer 
\begin_inset Formula $\beta$
\end_inset

 contains tokens remaining to be processed and the stack 
\begin_inset Formula $\sigma$
\end_inset

 contains partially processed tokens.
\end_layout

\begin_layout Standard
The transitions of one shift-reduce parser 
\begin_inset CommandInset citation
LatexCommand citep
key "nivre_efficient_2003"

\end_inset

 are:
\end_layout

\begin_layout Description

\noun on
Shift
\noun default
 removes the ﬁrst word 
\begin_inset Formula $w_{i}$
\end_inset

 in the buffer and pushes it on top of the stack.
\end_layout

\begin_layout Description

\noun on
Left-Arc(
\begin_inset Formula $r$
\end_inset

)
\noun default
 for any dependency label 
\begin_inset Formula $r$
\end_inset

.
 This transition has the effect of adding a dependency arc with label 
\begin_inset Formula $r$
\end_inset

 from the first token on the buffer 
\begin_inset Formula $w_{i}$
\end_inset

 to the token on the top of the stack 
\begin_inset Formula $w_{j}$
\end_inset

, and removing the top token from the stack (
\emph on
popping 
\emph default
the stack).
 
\end_layout

\begin_layout Description

\noun on
Right-Arc(
\begin_inset Formula $r$
\end_inset

)
\noun default
 again for any dependency label 
\begin_inset Formula $r$
\end_inset

.
 This transition works as 
\noun on
Left-Arc(
\begin_inset Formula $r$
\end_inset

)
\noun default
, except that in this case the introduced dependency arc goes the other
 way, from the token on the top of the stack to the first token on the buffer.
 In addition, the first token on the buffer is replaced by the token from
 the top of the stack.
\end_layout

\begin_layout Standard
The transitions have certain inevitable preconditions, e.g.
 
\noun on
Shift
\noun default
 is only permissible when the buffer is non-empty, and both 
\noun on
Arc
\noun default
 transitions only only makes sense when both the stack and the buffer are
 non-empty.
\end_layout

\begin_layout Standard
With an 
\emph on
initial state
\emph default
 
\begin_inset Formula $c_{0}=([w_{0}],[w_{1},w_{2},...,w_{n}],[])$
\end_inset

 with the first (artificial root) token on the stack, the rest of the sentence
 in the buffer and no arcs, the shift-reduce parser proceeds through a sequence
 of states 
\begin_inset Formula $C_{0,m}=(c_{o},c_{1},...,c_{m})$
\end_inset

 until it reaches the 
\emph on
terminal state
\emph default
 where the buffer is empty.
 At this point the dependency tree produced by the shift-reduce parser is
 contained in 
\begin_inset Formula $A$
\end_inset

.
\end_layout

\begin_layout Standard
Other transition-based approaches include list-based models 
\begin_inset CommandInset citation
LatexCommand citep
key "covington_fundamental_2001"

\end_inset

 and generalized LR-parsers 
\begin_inset CommandInset citation
LatexCommand citep
key "watson_adaptingrasp_2007,sagae_dependency_2007"

\end_inset

.
\end_layout

\begin_layout Standard
For all transition-based models, the training phase essentially consists
 of learning a model of what good transitions are given the parser state,
 which -- to reiterate -- involces both features of the tokens currently
 being processed and the previous transitions made.
\end_layout

\begin_layout Standard
TODO: runtime
\end_layout

\begin_layout Subsection
Graph-based models
\end_layout

\begin_layout Standard
Turning to another successful approach, the class of graph-based models,
 the learning task is concerned with what a good dependency graph is like,
 rather than which transitions are good.
\end_layout

\begin_layout Standard
Graph-based models define a scoring function over possible dependency graphs
 for a sentence, which is used at parse time in conjunction with a search
 algorithm to find for the highest-scoring graph.
 This score function should provide a measure 
\begin_inset Formula $\textrm{score}(G)$
\end_inset

 of the likelihood that a certain dependency graph 
\begin_inset Formula $G$
\end_inset

 is the correct analysis for the sentence 
\begin_inset Formula $S$
\end_inset

.
 
\end_layout

\begin_layout Subsubsection
Arc-factored models
\end_layout

\begin_layout Standard
In the training phase, graph based models learn the parameters of the scoring
 function.
 Different scoring functions and search algorithms have been proposed, all
 sharing the common property that the scoring function is decomposed into
 smaller functions that score local properties of the graph being evaluated,
 namely single attachments (for 
\emph on
first-order
\emph default
 or 
\emph on
arc-factored
\emph default
 models) or pairs of attachments (for 
\emph on
second-order
\emph default
 models).
 Typically, the score of a local property, be it a single arc or a pair,
 is calculated as a the dot product of a weight vector and the feature vector
 for the arc -- or in other words, a weighted sum of the arc features.
 
\end_layout

\begin_layout Standard
The score for the whole graph is typically found by summing the local property
 scores.
 The particular way in which subgraph scores is combined to achieve the
 score of the whole graph may differ between different graph-based models,
 but they share the fundamental assumption that the score of a graph 
\begin_inset Formula $G$
\end_inset

 factors through the scores of the subgraphs of 
\begin_inset Formula $G$
\end_inset

 
\begin_inset CommandInset citation
LatexCommand citep
key "kbler_dependency_2009"

\end_inset

.
 In the case of arc-factored models, where each arc is scored individually
 by the function 
\begin_inset Formula $\lambda(w_{i},r,w_{j})$
\end_inset

, we can write
\begin_inset Foot
status open

\begin_layout Plain Layout
Additionally, some models 
\begin_inset CommandInset citation
LatexCommand citep
key "nakagawa_multilingual_2007"

\end_inset

 include global properties of the graph in the scoring function.
 
\end_layout

\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula \[
\textrm{score}(G)=\sum\lambda(w_{i},r,w_{j})\]

\end_inset


\end_layout

\begin_layout Standard
In formal terms, the parsing algorithm 
\begin_inset Formula $h$
\end_inset

 in a graph-based model produces a dependency graph for a sentence 
\begin_inset Formula $S$
\end_inset

 by finding the graph the maximizes the scoring function:
\begin_inset Formula \[
h(S,\Gamma,\lambda)=\arg\max_{G\in\mathcal{G_{S}}}\textrm{score}(G)\]

\end_inset


\end_layout

\begin_layout Standard
The graph-theoretic term 
\emph on
spanning tree
\emph default
 denotes a subgraph 
\begin_inset Formula $G'=(V',A')$
\end_inset

 of a graph 
\begin_inset Formula $G=(V,A)$
\end_inset

 which satisfies the following criteria:
\end_layout

\begin_layout Enumerate

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\noun off
\color none
\begin_inset Formula $G'$
\end_inset


\family default
\series default
\shape default
\size default
\emph default
\bar default
\noun default
\color inherit
 is a directed tree.
\end_layout

\begin_layout Enumerate

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\noun off
\color none
\begin_inset Formula $G'$
\end_inset

 spans all the nodes of 
\begin_inset Formula $G$
\end_inset

, that is 
\begin_inset Formula $V=V'$
\end_inset

.
\end_layout

\begin_layout Standard
With a scoring function for subgraphs in the graph, the 
\emph on
maximum spanning tree
\emph default
 (MST) for a graph 
\begin_inset Formula $G$
\end_inset

 is simply the highest scoring spanning tree for 
\begin_inset Formula $G$
\end_inset

.
 Graph theory provides several algorithms for finding maximum spanning trees,
 which can be used if the parsing problem is cast as a graph search problem:
 Given the sentence 
\begin_inset Formula $S$
\end_inset

 and dependency types 
\begin_inset Formula $R$
\end_inset

, let 
\begin_inset Formula $G_{S}=(A_{S},V_{S})$
\end_inset

 denote the graph where the nodes 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\noun off
\color none

\begin_inset Formula $V_{S}$
\end_inset


\family default
\series default
\shape default
\size default
\emph default
\bar default
\noun default
\color inherit
 are the sentence tokens 
\begin_inset Formula $\{w_{0},w_{1},...,w_{n}\}$
\end_inset

 and there is an arc of all possible dependency types between all pairs
 of tokens, except for dependencies from the root token: 
\begin_inset Formula \[
A_{S}=\left\{ (w_{i},r,w_{j})\vert\textrm{ for all }w_{i},w_{j}\in S\textrm{ and }r\in R\textrm{ and }j\neq0\right\} \]

\end_inset

As 
\begin_inset Formula $G_{S}$
\end_inset

 has multiple arcs -- one for each dependency type 
\begin_inset Formula $r\in R$
\end_inset

 -- between each pair of nodes, and in both directions, it is a 
\emph on
multi digraph
\emph default
.
 However, in the case of arc-factored models, which score each arc individually,
 we can safely remove all but the highest scoring arc between any two nodes,
 thus reducing the multi digraph 
\begin_inset Formula $G_{S}$
\end_inset

 to a digraph 
\begin_inset Formula $G'_{S}=(V'_{S},A'_{S})$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{eqnarray*}
V'_{S} & = & V_{S}\\
A'_{S} & = & \left\{ (w_{i},w_{j})\vert w_{i},w_{j}\in V_{S},j\neq0\right\} \\
\lambda(w_{i},w_{j}) & = & \max_{r}\lambda(w_{i},r,w_{j})\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
This reduction is risk-free with arc-factored models, for the maximum arc
 scores we reduced to must include those of the maximum spanning tree.
 Otherwise, it would be possible to get a higher scoring tree by substituting
 arcs in it from 
\begin_inset Formula $A'_{S}$
\end_inset

.
 In order to recover the arc labels, it is necassary to keep track of the
 label 
\begin_inset Formula $r$
\end_inset

 that yielded the highest arc score 
\begin_inset Formula $\lambda(w_{i},r,w_{j})$
\end_inset

 for each pair of nodes.
\end_layout

\begin_layout Standard
The set of possible dependency trees for the sentence 
\begin_inset Formula $S$
\end_inset

 is identical to the set of spanning trees for 
\begin_inset Formula $G_{S}$
\end_inset

, and with the scoring function 
\begin_inset Formula $\textrm{score}(G)$
\end_inset

 we can get the best parse by finding the maximum spanning tree of 
\begin_inset Formula $G_{S}$
\end_inset

.
 A maximum spanning tree finding algorithm is Chu-Liu-Edmons, which proceeds
 in part with a greedy strategy by assigning the hightest scoring arcs to
 each node, and in part with a recursive procedure for untying possible
 loops produced in the greedy step.
 Given a sentence of length 
\begin_inset Formula $n$
\end_inset

, the Chu-Liu-Edmonds algorithm takes 
\begin_inset Formula $O(n^{3})$
\end_inset

 steps to find the maximum spanning tree, but with some trickery 
\begin_inset CommandInset citation
LatexCommand citep
key "tarjan_finding_1977"

\end_inset

 it can be made to run in 
\begin_inset Formula $O(n^{2})$
\end_inset

 steps.
 The algorithm works on the digraph 
\begin_inset Formula $G'_{S}$
\end_inset

 which must be constructed beforehand, yielding a total runtime of 
\begin_inset Formula $O(|R|n^{2}+n^{2})=O(|R|n^{2})$
\end_inset

.
\end_layout

\begin_layout Standard
TODO: learning
\end_layout

\begin_layout Section
State of the Art
\end_layout

\begin_layout Standard
CoNLL shared task.
 
\end_layout

\begin_layout Standard
MSTParser 
\begin_inset CommandInset citation
LatexCommand citep
key "mcdonald_multilingual_2006"

\end_inset

 and MaltParser 
\begin_inset CommandInset citation
LatexCommand citep
key "nivre_efficient_2003"

\end_inset

.
 
\end_layout

\begin_layout Standard
Languages.
\end_layout

\begin_layout Standard
Results.
 
\end_layout

\begin_layout Standard
Errors.
\end_layout

\begin_layout Chapter
Feature Engineering in Dependency Parsing
\end_layout

\begin_layout Section
Research Question
\end_layout

\begin_layout Standard
Possilbe to augment treebanks with additional or modified features such
 that existing data-driven parsing systems generate better dependency parsers?
\end_layout

\begin_layout Standard
In particular: Features to reduce errors from parentheticals.
 Itemization.
 Etc.
\end_layout

\begin_layout Section
Review of literature and research
\end_layout

\begin_layout Standard
What others have done.
\end_layout

\begin_layout Standard
In the original paper for the MSTParser, 
\begin_inset CommandInset citation
LatexCommand citet
key "mcdonald_multilingual_2006"

\end_inset

 note that the accuracy of verb and conjunction attachment for multiclause
 sentences in Spanish is well below the average:
\end_layout

\begin_layout Quotation
Although overall unlabeled accuracy is 86%, most verbs and some conjunctions
 attach to their head words with much lower accuracy: 69% for main verbs,
 75% for the verb 
\emph on
ser
\emph default
, and 65% for coordinating conjunctions.
 [...] These weaknesses are not surprising, since these decisions encode the
 more global aspects of sentence structure: arrangement of clauses and adverbial
 dependents in multi-clause sentences, and prepositional phrase attachment.
 
\end_layout

\begin_layout Standard
They describe prelimenary experiments with engineering a feature to improve
 the accuracy:
\end_layout

\begin_layout Quotation
[...] we added features to count the number of commas and conjunctions between
 a dependent verb and its candidate head.
 Unlabeled accuracy for all verbs increases from 71% to 73% and for all
 conjunctions from 71% to 74%.
 Unfortunately, accuracy for other word types decreases somewhat, resulting
 in no signiﬁcant net accuracy change.
 Nevertheless, this very preliminary experiment suggests that wider-range
 features may be useful in improv ing the recognition of overall sentence
 structure.
 
\end_layout

\begin_layout Standard

\noun on
Animacy
\noun default
 for disambiguation of subjects from objects.
 
\noun on
Definiteness
\noun default
 for disambiguation of subjects subject predicates.
 Verbal features (
\noun on
finiteness
\noun default
, 
\noun on
voice
\noun default
) for verbal dependency relations 
\begin_inset CommandInset citation
LatexCommand citep
key "vrelid_finite_2008,vrelid_linguistic_2008,vrelid_when_2007"

\end_inset

.
\end_layout

\begin_layout Standard
Chunking-like preprocessing step, identifying 
\begin_inset Quotes eld
\end_inset

blocks
\begin_inset Quotes erd
\end_inset

 
\begin_inset CommandInset citation
LatexCommand citep
key "zhou_block-based_2000"

\end_inset

.
\end_layout

\begin_layout Standard

\noun on
quotation
\noun default
s and 
\noun on
inserted clause
\noun default
s in spontaneous speech.
 Reduce dependencies erroneously crossing clause boundaries 
\begin_inset CommandInset citation
LatexCommand citep
key "hamabe_detection_2006"

\end_inset

.
\end_layout

\begin_layout Standard
Semantic super tagging.
 Segments: 
\begin_inset Quotes eld
\end_inset

there is exactly one dependency connecting a token outside the segmentwith
 a token inside the segment
\begin_inset Quotes erd
\end_inset

, and 
\begin_inset Quotes eld
\end_inset

segments tend to form units, with their own internal structure
\begin_inset Quotes erd
\end_inset

 
\begin_inset CommandInset citation
LatexCommand citep
key "ciaramita_dependency_2007"

\end_inset

.
 Second order feature maps.
\end_layout

\begin_layout Standard

\noun on
Capitalized
\noun default
, 
\noun on
coordinable
\noun default
, technical sufﬁces given by the lemmatizer 
\begin_inset CommandInset citation
LatexCommand citep
key "novk_feature_2007"

\end_inset

.
 Feature space reduction .
\end_layout

\begin_layout Standard
Bharati 
\begin_inset CommandInset citation
LatexCommand citet
key "bharati_two_2008"

\end_inset

: 
\noun on
Human-nonhuman
\noun default
 and 
\noun on
animate-inanimate
\noun default
 for 
\begin_inset Quotes eld
\end_inset

capturing argument structure of the verb, i.e.
 certain verbs can take only human subjects, etc.
\begin_inset Quotes erd
\end_inset

.
\end_layout

\begin_layout Standard
Base-NP chunking 
\begin_inset CommandInset citation
LatexCommand citep
key "tongchim_experiments_2008"

\end_inset

.
\end_layout

\begin_layout Standard
Chunking 
\begin_inset CommandInset citation
LatexCommand citep
key "attardi_chunking_2008"

\end_inset

.
\end_layout

\begin_layout Standard
Soft and hard constraints on dependency length 
\begin_inset CommandInset citation
LatexCommand citep
key "eisner_parsing_2005"

\end_inset

.
\end_layout

\begin_layout Section
Experiment Design
\end_layout

\begin_layout Standard
What I will do.
 Overview of proposed approach.
 
\end_layout

\begin_layout Subsection
Treebanks
\end_layout

\begin_layout Standard
Treebanks available: arabic bulgarian czech danish dutch english german
 italian japanese portuguese slovene spanish swedish turkish.
\end_layout

\begin_layout Standard
Qualities, statistics, appeal.
\end_layout

\begin_layout Subsection
Experiment setup
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename figures/flows/Experiment_Setup.eps
	lyxscale 50
	scale 50
	BoundingBox 0bp 0bp 861bp 598bp

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Treebank augmentation
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename figures/flows/Augmentation.eps
	lyxscale 50
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
System evaluation
\end_layout

\begin_layout Standard
Error analysis.
 Attachment scoring.
 Trouble with token-level scoring.
 Sentence-level scoring.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename figures/flows/Categorization.eps
	lyxscale 50
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename figures/flows/Evaluation.eps
	lyxscale 50
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Focused evaluation
\end_layout

\begin_layout Standard
Scoring categories of sentences: Those affected by augmentation, and those
 left untouched by the augmentation.
\end_layout

\begin_layout Subsection
Comparison to baseline
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename figures/flows/Comparison.eps
	lyxscale 50
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Statistical significance
\end_layout

\begin_layout Standard
“Stratified shuffling” 
\begin_inset CommandInset citation
LatexCommand citep
key "bikel_randomized_2004"

\end_inset

.
\end_layout

\begin_layout Section
Feature Designs
\end_layout

\begin_layout Standard
What I did.
\end_layout

\begin_layout Standard
[Present and discuss motivation, feature design and results for each focus
 area in turn]
\end_layout

\begin_layout Subsection
Parentheticals
\end_layout

\begin_layout Standard
Motivation: Word order in direct quoted speech 
\begin_inset CommandInset citation
LatexCommand citep
key "haberland_reported_1986"

\end_inset

.
 
\end_layout

\begin_layout Subsubsection

\noun on
Quotation
\end_layout

\begin_layout Standard
Feature designs: POS tags on quotation marks.
 Features on quoted tokens.
\end_layout

\begin_layout Standard
Results: Quotes in Danish, 
\end_layout

\begin_layout Standard
Discussion.
\end_layout

\begin_layout Subsubsection

\noun on
Parenthesis
\end_layout

\begin_layout Standard
Results: Italian.
\end_layout

\begin_layout Standard
FOOTNOTE: Det her viser meget godt de metodiske vanskeligheder i ikke at
 lave 10-fold CV på så lille en træbank: lemma only giver -.4% (forværring)
 på first fold (law).
 Du skal ikke bekymre dig om det, men kan måske skrive en fodnote om, at
 der er tale om en meget lille træbank, og du vælger at evaluere på EVALITA'07-s
etup'et for sammenlignelighed, selvom det er metodisk mistænkeligt.
 Skriv noget i retning af Chanev (2006) (bortset fra at hans engelsk er
 forfærdeligt): "Although the treebank is small and n-fold cross-validation
 is usually used in such cases, here we report results on a test set of
 150 sentences (4,172 tokens) and a training set of 1,350 sentences (37,444
 tokens) in order the TUT experiments not to differ from the experiments
 on the other treebanks in this study."
\end_layout

\begin_layout Subsubsection

\noun on
Colon
\end_layout

\begin_layout Standard
Results: Talbanken.
 Italian.
\end_layout

\begin_layout Subsection
Apposition
\end_layout

\begin_layout Standard
Results: Catalonian.
\end_layout

\begin_layout Subsection
List items
\end_layout

\begin_layout Standard
Results.
 Discussion.
\end_layout

\begin_layout Subsection
Coordinated enumerations
\end_layout

\begin_layout Standard
Results.
 Discussion.
\end_layout

\begin_layout Subsection
Lemmatization
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand citep
key "bart_cst_2008"

\end_inset


\end_layout

\begin_layout Standard
Results: Danish.
 
\end_layout

\begin_layout Standard
Discussion: Allows parser to generalize.
\end_layout

\begin_layout Subsection
Animacy
\end_layout

\begin_layout Standard
Results: Danish.
\end_layout

\begin_layout Standard
Discussion: Coverage and quality of animacy data.
 Head status of functional categories in DDT versus Talbanken.
 
\begin_inset CommandInset citation
LatexCommand citep
key "vrelid_cross-lingual_2009"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename /Users/christian/Projects/dep_feat/doc/figures/foci/animacy/ddt-heads.pdf

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
DDT 
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename /Users/christian/Projects/dep_feat/doc/figures/foci/animacy/talbanken05-heads.pdf

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Talbanken05
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Caption

\begin_layout Plain Layout
Different treatment of functional categories in DDT and in Talbanken05.
 In DDT, determiners -- such as 
\begin_inset Quotes eld
\end_inset

et
\begin_inset Quotes erd
\end_inset

 on the left hand side -- act as heads with nominal dependents, whereas
 Talbanken05
\emph on
 
\emph default
treats 
\emph on
nouns
\emph default
 as heads with functional dependents, as illustrated on the right hand side.
 The dependency structure examples are from Lilja Øvrelid's report on porting
 the animacy feature to Danish 
\begin_inset CommandInset citation
LatexCommand citep
key "vrelid_cross-lingual_2009"

\end_inset

.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Feature combinations
\end_layout

\begin_layout Standard
Effects of combining features.
\end_layout

\begin_layout Section
Results
\end_layout

\begin_layout Standard
Summarize results.
\end_layout

\begin_layout Chapter
Conclusion
\end_layout

\begin_layout Section
Future research
\end_layout

\begin_layout Standard
Expand on pronominalization, givenness and word order 
\begin_inset CommandInset citation
LatexCommand citep
key "weber_word_2004"

\end_inset

.
\end_layout

\begin_layout Chapter*
Acknowledgements
\end_layout

\begin_layout Itemize
Lilja Øvrelid (animacy tagger)
\end_layout

\begin_layout Itemize
Bart Jongejan (danish lemmatizer)
\end_layout

\begin_layout Itemize
Anders Søgaard (research idea, guidance)
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "export"
options "bibtotoc,plainnat"

\end_inset


\end_layout

\begin_layout Chapter
\start_of_appendix
Source code
\end_layout

\begin_layout Standard
Include?
\end_layout

\begin_layout Chapter
Example runtime output
\end_layout

\begin_layout Standard
Include?
\end_layout

\end_body
\end_document
